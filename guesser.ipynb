{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumdauo/dixit-AI-bot/blob/main/guesser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5-iuxmrXcEw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecl4cFKza7hV"
      },
      "source": [
        "# Extract CLIP embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1luuih3ta8Xk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "def extract_image_embeddings(cards_folder, device):\n",
        "    image_embeddings = []\n",
        "\n",
        "    for filename in sorted(os.listdir(cards_folder)):\n",
        "        if filename.endswith(('.png')):\n",
        "            image_path = os.path.join(cards_folder, filename)\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.get_image_features(**inputs)\n",
        "                embedding = embedding.squeeze().cpu()\n",
        "\n",
        "            if embedding_dim != embedding.shape[0]:\n",
        "                reduction_layer = torch.nn.Linear(embedding.shape[0], embedding_dim).to(device)\n",
        "                embedding = reduction_layer(embedding.to(device)).cpu()\n",
        "\n",
        "            image_embeddings.append(embedding)\n",
        "\n",
        "    all_image_embeddings = torch.stack(image_embeddings)\n",
        "    return all_image_embeddings\n",
        "\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "embeddings_save_path = config[\"embeddings_save_path\"]\n",
        "clip_model_name = config[\"clip_model_name\"]\n",
        "embedding_dim = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "all_image_embeddings = extract_image_embeddings(cards_folder, device)\n",
        "torch.save(all_image_embeddings, embeddings_save_path)\n",
        "print(f\"Image embeddings saved at {embeddings_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S24nfWDabpU"
      },
      "source": [
        "# Llama, Dixit model<br>\n",
        "Performing the guesser phase with random cards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvON7ISnaacI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    CLIPProcessor,\n",
        "    CLIPModel,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM\n",
        ")\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "class DixitModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout_rate=0.5):\n",
        "        super(DixitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(256, embedding_dim)\n",
        "\n",
        "    def forward(self, hint_embedding):\n",
        "        x = F.relu(self.bn1(self.fc1(hint_embedding)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DixitAI:\n",
        "    def __init__(self, descriptions_file_path, checkpoint_path, embeddings_save_path, llama_model_path, hf_token):\n",
        "        self.card_descriptions = {}\n",
        "        descriptions_df = pd.read_csv(descriptions_file_path)\n",
        "        for _, row in descriptions_df.iterrows():\n",
        "            card_id = re.search(r'\\d+', row['Image']).group()\n",
        "            self.card_descriptions[card_id] = {\n",
        "                'blip_description': row['BLIP'],\n",
        "                'vit_description': row['ViT'],\n",
        "                'blip2_description': row['BLIP-2']\n",
        "            }\n",
        "\n",
        "        embedding_dim = 512\n",
        "        self.dixit_model = DixitModel(embedding_dim=embedding_dim).to(device)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        self.dixit_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.dixit_model.eval()\n",
        "\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        self.llama_tokenizer = LlamaTokenizer.from_pretrained(llama_model_path, use_auth_token=hf_token)\n",
        "        self.llama_model = LlamaForCausalLM.from_pretrained(\n",
        "            llama_model_path,\n",
        "            use_auth_token=hf_token,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(device)\n",
        "        self.llama_model.eval()\n",
        "\n",
        "        self.all_image_embeddings = torch.load(embeddings_save_path, map_location=device)\n",
        "        self.num_cards = self.all_image_embeddings.shape[0]\n",
        "\n",
        "    def create_cot_prompt(self, hint, candidate_cards):\n",
        "        prompt_template = f\"\"\"You are an expert at matching images to a hint and giving a relatedness score.\n",
        "            Your task is to evaluate each card's relevance to the given hint by giving a score.\n",
        "\n",
        "            Hint: '{hint}'\n",
        "\n",
        "            \"\"\"\n",
        "        for i, card_id in enumerate(candidate_cards):\n",
        "            if 0 <= card_id < self.num_cards:\n",
        "                blip_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip_description', \"No description available\")\n",
        "                vit_desc = self.card_descriptions.get(str(card_id + 1), {}).get('vit_description', \"No description available\")\n",
        "                blip2_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip2_description', \"No description available\")\n",
        "\n",
        "                prompt_template += f\"\"\"\n",
        "                Card {card_id + 1}:\n",
        "                Descriptions: BLIP - {blip_desc} | ViT - {vit_desc} | BLIP-2 - {blip2_desc}\n",
        "                Provide a numeric relatedness score (0-10) between image and hint.\n",
        "                \"\"\"\n",
        "\n",
        "        return prompt_template\n",
        "\n",
        "    def score_card(self, hint, card_id):\n",
        "        if not (0 <= card_id < self.num_cards):\n",
        "            raise ValueError(f\"Invalid card_id: {card_id}. Must be between 0 and {self.num_cards-1}\")\n",
        "\n",
        "        hint_inputs = self.clip_processor(text=hint, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            hint_embedding = self.clip_model.get_text_features(**hint_inputs)\n",
        "            hint_embedding = F.normalize(hint_embedding, p=2, dim=-1).cpu()\n",
        "\n",
        "        hint_embedding = self.dixit_model(hint_embedding.to(device)).squeeze(0)\n",
        "        hint_embedding = F.normalize(hint_embedding, p=2, dim=0)\n",
        "\n",
        "        card_embedding = F.normalize(self.all_image_embeddings[card_id], p=2, dim=0)\n",
        "        similarity = F.cosine_similarity(hint_embedding, card_embedding, dim=0).item()\n",
        "        return similarity\n",
        "\n",
        "    def generate_reasoning(self, hint, candidate_cards):\n",
        "        \"\"\"Generate longer reasoning using LLaMA with clear instructions for output.\"\"\"\n",
        "        prompt = self.create_cot_prompt(hint, candidate_cards)\n",
        "\n",
        "        input_ids = self.llama_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        max_length = min(input_ids.shape[1] + 200, 2048)\n",
        "\n",
        "        output = self.llama_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            top_k=20,\n",
        "            top_p=0.6,  # decrease top_p to increase output diversity\n",
        "            temperature=0.3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=self.llama_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        response = self.llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        reasoning_output = response[len(prompt):].strip()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*30 + \" DEBUG: LLaMA Reasoning Output \" + \"=\"*30)\n",
        "        print(reasoning_output)\n",
        "        print(\"=\"*75 + \"\\n\")\n",
        "\n",
        "        score_match = re.search(r'score:\\s*(\\d+)/10\\b', reasoning_output, re.IGNORECASE)\n",
        "\n",
        "        if score_match:\n",
        "            score_value = float(score_match.group(1))\n",
        "        else:\n",
        "            score_match = re.search(r'\\b([2-9])\\b', reasoning_output)\n",
        "            if score_match:\n",
        "                preceding_text = reasoning_output[:score_match.start()]\n",
        "                score_value = float(score_match.group(1))\n",
        "            else:\n",
        "                return reasoning_output, 0.0\n",
        "\n",
        "        print(score_match)\n",
        "\n",
        "        normalized_score = (score_value - 5) / 5\n",
        "        return reasoning_output, normalized_score\n",
        "\n",
        "    def choose_card(self, hint, candidate_cards):\n",
        "        scores = {}\n",
        "        for card_id in candidate_cards:\n",
        "            similarity = self.score_card(hint, card_id)\n",
        "            print(f\"Card {card_id + 1} Similarity Score: {similarity:.3f}\")\n",
        "            reasoning, reasoning_score = self.generate_reasoning(hint, [card_id])\n",
        "            combined_score = 0.7 * reasoning_score + 0.3 * similarity\n",
        "            scores[card_id] = combined_score\n",
        "\n",
        "        best_card = max(scores.items(), key=lambda x: x[1])[0]\n",
        "        return best_card, scores[best_card]\n",
        "\n",
        "    def play_turn(self, hint):\n",
        "        \"\"\"Play a complete turn as both card selector and guesser\"\"\"\n",
        "        # First subphase: bot selects a card from its own hand\n",
        "        bot_hand = random.sample(range(self.num_cards), 6)\n",
        "        chosen_card, score = self.choose_card(hint, bot_hand)\n",
        "        print(f\"Selected card {chosen_card + 1} from hand with score {score:.3f} from {bot_hand}\" )\n",
        "\n",
        "        # Second subphase: other players' cards for guessing\n",
        "        available_cards = [i for i in range(self.num_cards) if i not in bot_hand]\n",
        "        other_cards = random.sample(available_cards, 3)\n",
        "\n",
        "        print(f\"\\nOther players' cards: {[card + 1 for card in other_cards]}\")\n",
        "\n",
        "        # Bot guesses the target card among other players' cards\n",
        "        guess_card, guess_score = self.choose_card(hint, other_cards)\n",
        "        print(f\"Guessed card {guess_card + 1} with score {guess_score:.3f}\")\n",
        "\n",
        "        return chosen_card, guess_card\n",
        "\n",
        "def main():\n",
        "    dixit_ai = DixitAI(\n",
        "        descriptions_file_path=config[\"descriptions_file_path\"],\n",
        "        checkpoint_path=config[\"checkpoint_path\"],\n",
        "        embeddings_save_path=config[\"embeddings_save_path\"],\n",
        "        llama_model_path=config[\"llama_model_path\"],\n",
        "        hf_token=config[\"hf_token\"]\n",
        "    )\n",
        "\n",
        "    hint = \"Looking forward to meet you\"\n",
        "    chosen_card, guessed_card = dixit_ai.play_turn(hint)\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Selected card: {chosen_card + 1}\")\n",
        "    print(f\"Guessed card: {guessed_card + 1}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "embeddings_save_path = config[\"embeddings_save_path\"]\n",
        "clip_model_name = config[\"clip_model_name\"]\n",
        "descriptions_file_path = config[\"descriptions_file_path\"]\n",
        "checkpoint_path = config[\"checkpoint_path\"]\n",
        "llama_model_path = config[\"llama_model_path\"]\n",
        "hf_token = config[\"hf_token\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_UNEDSI6icP"
      },
      "source": [
        "# Llama, Dixit model <br>\n",
        "Performing the guesser phase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8G71rF06n-_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    CLIPProcessor,\n",
        "    CLIPModel,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM\n",
        ")\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "class DixitModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout_rate=0.5):\n",
        "        super(DixitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(256, embedding_dim)\n",
        "\n",
        "    def forward(self, hint_embedding):\n",
        "        x = F.relu(self.bn1(self.fc1(hint_embedding)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DixitAI:\n",
        "    def __init__(self, descriptions_file_path, checkpoint_path, embeddings_save_path, llama_model_path, hf_token):\n",
        "        self.card_descriptions = {}\n",
        "        descriptions_df = pd.read_csv(descriptions_file_path)\n",
        "        for _, row in descriptions_df.iterrows():\n",
        "            card_id = re.search(r'\\d+', row['Image']).group()\n",
        "            self.card_descriptions[card_id] = {\n",
        "                'blip_description': row['BLIP'],\n",
        "                'vit_description': row['ViT'],\n",
        "                'blip2_description': row['BLIP-2']\n",
        "            }\n",
        "\n",
        "        embedding_dim = 512\n",
        "        self.dixit_model = DixitModel(embedding_dim=embedding_dim).to(device)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        self.dixit_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.dixit_model.eval()\n",
        "\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        self.llama_tokenizer = LlamaTokenizer.from_pretrained(llama_model_path, use_auth_token=hf_token)\n",
        "        self.llama_model = LlamaForCausalLM.from_pretrained(\n",
        "            llama_model_path,\n",
        "            use_auth_token=hf_token,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(device)\n",
        "        self.llama_model.eval()\n",
        "\n",
        "        self.all_image_embeddings = torch.load(embeddings_save_path, map_location=device)\n",
        "        self.num_cards = self.all_image_embeddings.shape[0]\n",
        "\n",
        "    def create_cot_prompt(self, hint, candidate_cards):\n",
        "        prompt_template = f\"\"\"You are an expert at matching images to a hint and giving a relatedness score.\n",
        "            Your task is to evaluate each card's relevance to the given hint by giving a score.\n",
        "\n",
        "            Hint: '{hint}'\n",
        "\n",
        "            \"\"\"\n",
        "        for i, card_id in enumerate(candidate_cards):\n",
        "            if 0 <= card_id < self.num_cards:\n",
        "                blip_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip_description', \"No description available\")\n",
        "                vit_desc = self.card_descriptions.get(str(card_id + 1), {}).get('vit_description', \"No description available\")\n",
        "                blip2_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip2_description', \"No description available\")\n",
        "\n",
        "                prompt_template += f\"\"\"\n",
        "                Card {card_id + 1}:\n",
        "                Descriptions: BLIP - {blip_desc} | ViT - {vit_desc} | BLIP-2 - {blip2_desc}\n",
        "                Provide a relatedness score (0-10) between image and hint.\n",
        "                Score:\n",
        "                \"\"\"\n",
        "\n",
        "        return prompt_template\n",
        "\n",
        "    def score_card(self, hint, card_id):\n",
        "        if not (0 <= card_id < self.num_cards):\n",
        "            raise ValueError(f\"Invalid card_id: {card_id}. Must be between 0 and {self.num_cards-1}\")\n",
        "\n",
        "        hint_inputs = self.clip_processor(text=hint, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            hint_embedding = self.clip_model.get_text_features(**hint_inputs)\n",
        "            hint_embedding = F.normalize(hint_embedding, p=2, dim=-1).cpu()\n",
        "\n",
        "        hint_embedding = self.dixit_model(hint_embedding.to(device)).squeeze(0)\n",
        "        hint_embedding = F.normalize(hint_embedding, p=2, dim=0)\n",
        "\n",
        "        card_embedding = F.normalize(self.all_image_embeddings[card_id], p=2, dim=0)\n",
        "        similarity = F.cosine_similarity(hint_embedding, card_embedding, dim=0).item()\n",
        "        return similarity\n",
        "\n",
        "    def generate_reasoning(self, hint, candidate_cards):\n",
        "        \"\"\"Generate reasoning using LLaMA with to get responses and scores.\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"You are an expert at evaluating the relevance of images to a hint by analyzing descriptions.\n",
        "        Your task is to assess each image's relevance to the hint and give a relatedness score (0-10).\n",
        "\n",
        "        Hint: '{hint}'\n",
        "\n",
        "        For each candidate image, analyze its descriptions and follow these instructions:\n",
        "        1. Write a brief reasoning explaining the relevance or irrelevance of the descriptions to the hint.\n",
        "        2. On a new line, provide a \"Score:\" followed by a numeric score (0-10), with 10 being the most relevant and 0 the least.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for i, card_id in enumerate(candidate_cards):\n",
        "            if 0 <= card_id < self.num_cards:\n",
        "                blip_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip_description', \"No description available\")\n",
        "                vit_desc = self.card_descriptions.get(str(card_id + 1), {}).get('vit_description', \"No description available\")\n",
        "                blip2_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip2_description', \"No description available\")\n",
        "\n",
        "                prompt += f\"\"\"\n",
        "                Description for Card {card_id + 1}: {blip_desc}, {vit_desc}, {blip2_desc}\n",
        "                Reasoning and Score:\n",
        "                \"\"\"\n",
        "\n",
        "        input_ids = self.llama_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        max_length = min(input_ids.shape[1] + 500, 2048)\n",
        "\n",
        "        output = self.llama_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            temperature=0.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=self.llama_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        response = self.llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        reasoning_output = response[len(prompt):].strip()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*30 + \"LLaMA Reasoning Output\" + \"=\"*30)\n",
        "        print(\"Reasoning Output:\\n\", reasoning_output)\n",
        "        print(\"=\"*75 + \"\\n\")\n",
        "\n",
        "        score_match = re.search(r'score:\\s*(\\d+)/10\\b', reasoning_output, re.IGNORECASE)\n",
        "\n",
        "        if score_match:\n",
        "            score_value = float(score_match.group(1))\n",
        "        else:\n",
        "            score_match = re.search(r'\\b([2-9])\\b', reasoning_output)\n",
        "            if score_match:\n",
        "                preceding_text = reasoning_output[:score_match.start()]\n",
        "                score_value = float(score_match.group(1))\n",
        "            else:\n",
        "                return reasoning_output, 0.0\n",
        "\n",
        "        normalized_score = (score_value - 5) / 5\n",
        "        print(\"Normalized Score:\", normalized_score)\n",
        "        return reasoning_output, normalized_score\n",
        "\n",
        "\n",
        "    def choose_card(self, hint, candidate_cards):\n",
        "        \"\"\"Choose the best card from the candidates based on the hint.\"\"\"\n",
        "        scores = {}\n",
        "        for card_id in candidate_cards:\n",
        "            similarity = self.score_card(hint, card_id)\n",
        "            print(f\"Card {card_id + 1} Similarity Score: {similarity:.3f}\")\n",
        "            reasoning, reasoning_score = self.generate_reasoning(hint, [card_id])\n",
        "            combined_score = 0.7 * reasoning_score + 0.3 * similarity\n",
        "            scores[card_id] = combined_score\n",
        "\n",
        "        best_card = max(scores.items(), key=lambda x: x[1])[0]\n",
        "        return best_card, scores[best_card]\n",
        "\n",
        "    def play_turn(self, hint):\n",
        "        \"\"\"Play a complete turn as both card selector and guesser.\"\"\"\n",
        "        bot_hand = input(\"Enter six card IDs for the bot's hand, separated by spaces: \")\n",
        "        bot_hand = [int(card_id) - 1 for card_id in bot_hand.split()]\n",
        "\n",
        "        chosen_card, score = self.choose_card(hint, bot_hand)\n",
        "        print(f\"Selected card {chosen_card + 1} from hand with score {score:.3f} from {bot_hand}\")\n",
        "\n",
        "        other_cards = input(\"Enter three card IDs for other players' cards, separated by spaces: \")\n",
        "        other_cards = [int(card_id) - 1 for card_id in other_cards.split()]\n",
        "\n",
        "        guess_card, guess_score = self.choose_card(hint, other_cards)\n",
        "        print(f\"Guessed card {guess_card + 1} with score {guess_score:.3f}\")\n",
        "\n",
        "        return chosen_card, guess_card\n",
        "\n",
        "def main():\n",
        "    dixit_ai = DixitAI(\n",
        "        descriptions_file_path=config[\"descriptions_file_path\"],\n",
        "        checkpoint_path=config[\"checkpoint_path\"],\n",
        "        embeddings_save_path=config[\"embeddings_save_path\"],\n",
        "        llama_model_path=config[\"llama_model_path\"],\n",
        "        hf_token=config[\"hf_token\"]\n",
        "    )\n",
        "\n",
        "    hint = input(\"Enter a hint for this turn: \")\n",
        "    chosen_card, guessed_card = dixit_ai.play_turn(hint)\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Selected card: {chosen_card + 1}\")\n",
        "    print(f\"Guessed card: {guessed_card + 1}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "embeddings_save_path = config[\"embeddings_save_path\"]\n",
        "clip_model_name = config[\"clip_model_name\"]\n",
        "descriptions_file_path = config[\"descriptions_file_path\"]\n",
        "checkpoint_path = config[\"checkpoint_path\"]\n",
        "llama_model_path = config[\"llama_model_path\"]\n",
        "hf_token = config[\"hf_token\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd7KD_3Ahmkf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    CLIPProcessor,\n",
        "    CLIPModel,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM\n",
        ")\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "class DixitModel(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(DixitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, embedding_dim)\n",
        "\n",
        "    def forward(self, hint_embedding):\n",
        "        x = F.relu(self.bn1(self.fc1(hint_embedding)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "class DixitAI:\n",
        "    def __init__(self, descriptions_file_path, checkpoint_path, embeddings_save_path, llama_model_path, hf_token):\n",
        "        self.card_descriptions = {}\n",
        "        descriptions_df = pd.read_csv(descriptions_file_path)\n",
        "        for _, row in descriptions_df.iterrows():\n",
        "            card_id = re.search(r'\\d+', row['Image']).group()\n",
        "            self.card_descriptions[card_id] = {\n",
        "                'blip_description': row['BLIP'],\n",
        "                'vit_description': row['ViT'],\n",
        "                'blip2_description': row['BLIP-2']\n",
        "            }\n",
        "\n",
        "        embedding_dim = 512\n",
        "        self.dixit_model = DixitModel(embedding_dim=embedding_dim).to(device)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        self.dixit_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.dixit_model.eval()\n",
        "\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        self.llama_tokenizer = LlamaTokenizer.from_pretrained(llama_model_path, use_auth_token=hf_token)\n",
        "        self.llama_model = LlamaForCausalLM.from_pretrained(\n",
        "            llama_model_path,\n",
        "            use_auth_token=hf_token,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            low_cpu_mem_usage=True\n",
        "        ).to(device)\n",
        "        self.llama_model.eval()\n",
        "\n",
        "        self.all_image_embeddings = torch.load(embeddings_save_path, map_location=device)\n",
        "        self.num_cards = self.all_image_embeddings.shape[0]\n",
        "\n",
        "    def score_card(self, hint, card_id):\n",
        "        \"\"\"Compute similarity between transformed hint and card embeddings.\"\"\"\n",
        "        if not (0 <= card_id < self.num_cards):\n",
        "            raise ValueError(f\"Invalid card_id: {card_id}. Must be between 0 and {self.num_cards-1}\")\n",
        "\n",
        "        hint_inputs = self.clip_processor(text=hint, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            hint_embedding = self.clip_model.get_text_features(**hint_inputs)\n",
        "            hint_embedding = F.normalize(hint_embedding, p=2, dim=-1).cpu()\n",
        "\n",
        "        hint_embedding = self.dixit_model(hint_embedding.to(device)).squeeze(0)\n",
        "        hint_embedding = F.normalize(hint_embedding, p=2, dim=0)\n",
        "\n",
        "        card_embedding = F.normalize(self.all_image_embeddings[card_id], p=2, dim=0)\n",
        "        similarity = F.cosine_similarity(hint_embedding, card_embedding, dim=0).item()\n",
        "        print(\"\\n\" + \"=\"*30 + \"=\"*30)\n",
        "        return similarity\n",
        "\n",
        "    def generate_reasoning(self, hint, candidate_cards):\n",
        "        \"\"\"Generate reasoning using LLaMA to get responses and scores.\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"You are an expert at evaluating the relevance of images to a hint by analyzing descriptions.\n",
        "        Your task is to assess each image's relevance to the hint and give a relatedness score (0-10).\n",
        "\n",
        "        Hint: '{hint}'\n",
        "\n",
        "        For each candidate image, analyze its descriptions and follow these instructions:\n",
        "        1. Write a brief reasoning explaining the relevance or irrelevance of the descriptions to the hint.\n",
        "        2. On a new line, provide a \"Score:\" followed by a numeric score (0-10), with 10 being the most relevant and 0 the least.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for i, card_id in enumerate(candidate_cards):\n",
        "            if 0 <= card_id < self.num_cards:\n",
        "                blip_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip_description', \"No description available\")\n",
        "                vit_desc = self.card_descriptions.get(str(card_id + 1), {}).get('vit_description', \"No description available\")\n",
        "                blip2_desc = self.card_descriptions.get(str(card_id + 1), {}).get('blip2_description', \"No description available\")\n",
        "\n",
        "                prompt += f\"\"\"\n",
        "                Description for Card {card_id + 1}: {blip_desc}, {vit_desc}, {blip2_desc}\n",
        "                Reasoning and Score:\n",
        "                \"\"\"\n",
        "\n",
        "        input_ids = self.llama_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "        max_length = min(input_ids.shape[1] + 500, 2048)\n",
        "\n",
        "        output = self.llama_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            temperature=0.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=self.llama_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        response = self.llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        reasoning_output = response[len(prompt):].strip()\n",
        "\n",
        "        print(\"Reasoning Output:\\n\", reasoning_output)\n",
        "\n",
        "        score_match = re.search(r'score:\\s*(\\d+)/10\\b', reasoning_output, re.IGNORECASE)\n",
        "\n",
        "        if score_match:\n",
        "            score_value = float(score_match.group(1))\n",
        "        else:\n",
        "            score_match = re.search(r'\\b([2-9]|10)\\b(?!\\.)', reasoning_output)\n",
        "            if score_match:\n",
        "                preceding_text = reasoning_output[:score_match.start()]\n",
        "                score_value = float(score_match.group(1))\n",
        "            else:\n",
        "                return reasoning_output, 0.0\n",
        "\n",
        "        normalized_score = (score_value - 5) / 5\n",
        "        print(\"Normalized Score:\", normalized_score)\n",
        "        return reasoning_output, normalized_score\n",
        "\n",
        "\n",
        "    def choose_card(self, hint, candidate_cards):\n",
        "        \"\"\"Choose the best card from the candidates based on the hint.\"\"\"\n",
        "        scores = {}\n",
        "        for card_id in candidate_cards:\n",
        "            similarity = self.score_card(hint, card_id)\n",
        "            print(f\"Card {card_id + 1} Similarity Score: {similarity:.3f}\")\n",
        "            reasoning, reasoning_score = self.generate_reasoning(hint, [card_id])\n",
        "            combined_score = 0.6 * reasoning_score + 0.4 * similarity\n",
        "            print(f\"Combined Score: {combined_score:.3f}\")\n",
        "            scores[card_id] = combined_score\n",
        "\n",
        "        best_card = max(scores.items(), key=lambda x: x[1])[0]\n",
        "        return best_card, scores[best_card]\n",
        "\n",
        "    def play_turn(self, hint):\n",
        "        \"\"\"Play a complete turn as both card selector and guesser.\"\"\"\n",
        "        bot_hand = input(\"Enter six card IDs for the bot's hand, separated by spaces: \")\n",
        "        bot_hand = [int(card_id) - 1 for card_id in bot_hand.split()]\n",
        "\n",
        "        chosen_card, score = self.choose_card(hint, bot_hand)\n",
        "        print(f\"Selected card {chosen_card + 1} from hand with score {score:.3f} from {bot_hand}\")\n",
        "\n",
        "        other_cards = input(\"Enter three card IDs for other players' cards, separated by spaces: \")\n",
        "        other_cards = [int(card_id) - 1 for card_id in other_cards.split()]\n",
        "\n",
        "        guess_card, guess_score = self.choose_card(hint, other_cards)\n",
        "        print(f\"Guessed card {guess_card + 1} with score {guess_score:.3f}\")\n",
        "\n",
        "        return chosen_card, guess_card\n",
        "\n",
        "def main():\n",
        "    dixit_ai = DixitAI(\n",
        "        descriptions_file_path=config[\"descriptions_file_path\"],\n",
        "        checkpoint_path=config[\"checkpoint_path\"],\n",
        "        embeddings_save_path=config[\"embeddings_save_path\"],\n",
        "        llama_model_path=config[\"llama_model_path\"],\n",
        "        hf_token=config[\"hf_token\"]\n",
        "    )\n",
        "\n",
        "    hint = input(\"Enter a hint for this turn: \")\n",
        "    chosen_card, guessed_card = dixit_ai.play_turn(hint)\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Selected card: {chosen_card + 1}\")\n",
        "    print(f\"Guessed card: {guessed_card + 1}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "embeddings_save_path = config[\"embeddings_save_path\"]\n",
        "clip_model_name = config[\"clip_model_name\"]\n",
        "descriptions_file_path = config[\"descriptions_file_path\"]\n",
        "checkpoint_path = config[\"checkpoint_path_contrastiveloss\"]\n",
        "llama_model_path = config[\"llama_model_path\"]\n",
        "hf_token = config[\"hf_token\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOD2q9b1NTMgkRIuKhZGrub",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}