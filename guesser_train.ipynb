{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumdauo/dixit-AI-bot/blob/main/guesser_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5liSuQuAaMlq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czg5K_dHapu4"
      },
      "source": [
        "# Extract images embeddings with CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27fERyZaaqjA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "def extract_image_embeddings(cards_folder):\n",
        "    image_embeddings = {}\n",
        "    for filename in os.listdir(cards_folder):\n",
        "        if filename.endswith(('.png')):\n",
        "            card_id = int(os.path.splitext(filename)[0])\n",
        "            image_path = os.path.join(cards_folder, filename)\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.get_image_features(**inputs)\n",
        "                embedding = F.normalize(embedding, p=2, dim=-1).cpu()\n",
        "            image_embeddings[card_id - 1] = embedding  # Adjust to 0-based index\n",
        "\n",
        "    return image_embeddings\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser_train.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "embeddings_path = config[\"embeddings_path\"]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "clip_model = CLIPModel.from_pretrained(config[\"model_name\"]).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(config[\"model_name\"])\n",
        "\n",
        "image_embeddings = extract_image_embeddings(cards_folder)\n",
        "with open(embeddings_path, 'wb') as f:\n",
        "    pickle.dump(image_embeddings, f)\n",
        "\n",
        "print(f\"Image embeddings saved to {embeddings_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G49wqlj6dngc"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NObzrO6eNXhe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with open(embeddings_path, 'rb') as f:\n",
        "    image_embeddings = pickle.load(f)\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "class DixitDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_embeddings, debug_size=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        if debug_size:\n",
        "            self.data = self.data.sample(n=debug_size, random_state=42)\n",
        "        self.image_embeddings = image_embeddings\n",
        "        self.truncated_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def keyword_extract(self, text):\n",
        "        words = re.findall(r'\\w+', text.lower())\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        most_common_words = [word for word, _ in Counter(words).most_common(77)]\n",
        "        return ' '.join(most_common_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hint = self.data.iloc[idx]['DESCRIPTION']\n",
        "        target = int(self.data.iloc[idx]['TARGET']) - 1\n",
        "        distractor_idxs = [idx for idx in range(len(self.image_embeddings)) if idx != target]\n",
        "        distractor = self.image_embeddings[np.random.choice(distractor_idxs)]\n",
        "\n",
        "        hint = self.keyword_extract(hint)\n",
        "        hint_inputs = clip_processor(\n",
        "            text=hint,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hint_embedding = clip_model.get_text_features(**hint_inputs)\n",
        "            hint_embedding = F.normalize(hint_embedding, p=2, dim=-1).cpu()\n",
        "\n",
        "        target_embedding = self.image_embeddings[target]\n",
        "        return hint_embedding.squeeze(0), target_embedding.squeeze(0), distractor.squeeze(0)\n",
        "\n",
        "class DixitModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout_rate):\n",
        "        super(DixitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, embedding_dim)\n",
        "\n",
        "    def forward(self, hint_embedding):\n",
        "        x = F.relu(self.fc1(hint_embedding))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    torch.save(state, os.path.join(checkpoint_dir, filename))\n",
        "    if is_best:\n",
        "        torch.save(state, os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "        best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch})\")\n",
        "        return start_epoch, best_val_loss\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from scratch.\")\n",
        "        return 0, float('inf')\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return F.cosine_similarity(a, b, dim=-1)\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_similarities = []\n",
        "\n",
        "    for hint_embedding, target_embedding, distractor_embedding in tqdm(dataloader):\n",
        "        hint_embedding = hint_embedding.to(device)\n",
        "        target_embedding = target_embedding.to(device)\n",
        "        distractor_embedding = distractor_embedding.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor = model(hint_embedding)\n",
        "        loss = criterion(anchor, target_embedding, distractor_embedding)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        similarity_to_target = cosine_similarity(anchor, target_embedding)\n",
        "        similarity_to_distractor = cosine_similarity(anchor, distractor_embedding)\n",
        "\n",
        "        correct_predictions = (similarity_to_target > similarity_to_distractor).sum().item()\n",
        "        total_correct += correct_predictions\n",
        "        total_samples += hint_embedding.size(0)\n",
        "\n",
        "        all_similarities.append(similarity_to_target.mean().item() - similarity_to_distractor.mean().item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_similarity_diff = np.mean(all_similarities)\n",
        "\n",
        "    return avg_loss, accuracy, avg_similarity_diff\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for hint_embedding, target_embedding, distractor_embedding in dataloader:\n",
        "            hint_embedding = hint_embedding.to(device)\n",
        "            target_embedding = target_embedding.to(device)\n",
        "            distractor_embedding = distractor_embedding.to(device)\n",
        "\n",
        "            anchor = model(hint_embedding)\n",
        "            loss = criterion(anchor, target_embedding, distractor_embedding)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            similarity_to_target = cosine_similarity(anchor, target_embedding)\n",
        "            similarity_to_distractor = cosine_similarity(anchor, distractor_embedding)\n",
        "\n",
        "            correct_predictions = (similarity_to_target > similarity_to_distractor).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += hint_embedding.size(0)\n",
        "\n",
        "            all_similarities.append(similarity_to_target.mean().item() - similarity_to_distractor.mean().item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_similarity_diff = np.mean(all_similarities)\n",
        "\n",
        "    return avg_loss, accuracy, avg_similarity_diff\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser_train.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "csv_path = config[\"csv_path\"]\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "checkpoint_dir = config[\"checkpoint_dir\"]\n",
        "embeddings_path = config[\"embeddings_path\"]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 50\n",
        "DROPOUT_RATE = 0.6  # Increased from 0.5\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "WEIGHT_DECAY = 1e-5\n",
        "TRIPLET_MARGIN = 0.4  # Increased from 0.3\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(config[\"model_name\"]).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(config[\"model_name\"])\n",
        "\n",
        "dataset = DixitDataset(csv_path, image_embeddings)\n",
        "train_size = int((1 - VALIDATION_SPLIT) * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "embedding_dim = next(iter(train_loader))[0].shape[1]\n",
        "model = DixitModel(embedding_dim, DROPOUT_RATE).to(device)\n",
        "criterion = nn.TripletMarginLoss(margin=TRIPLET_MARGIN)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "start_epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_dir)\n",
        "\n",
        "early_stopping_counter = 0\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    train_loss, train_accuracy, train_similarity_diff = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy, val_similarity_diff = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Training Similarity Difference: {train_similarity_diff:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
        "          f\"Validation Similarity Difference: {val_similarity_diff:.4f}\")\n",
        "\n",
        "    is_best = val_loss < best_val_loss\n",
        "    if is_best:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss\n",
        "    }, is_best, checkpoint_dir)\n",
        "\n",
        "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXhqXp58FLga",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "class DixitDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_embeddings, debug_size=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        if debug_size:\n",
        "            self.data = self.data.sample(n=debug_size, random_state=42)\n",
        "        self.image_embeddings = image_embeddings\n",
        "        self.truncated_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def keyword_extract(self, text):\n",
        "        words = re.findall(r'\\w+', text.lower())\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        most_common_words = [word for word, _ in Counter(words).most_common(77)]\n",
        "        return ' '.join(most_common_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hint = self.data.iloc[idx]['DESCRIPTION']\n",
        "        target = int(self.data.iloc[idx]['TARGET']) - 1\n",
        "        distractor_idxs = [idx for idx in range(len(self.image_embeddings)) if idx != target]\n",
        "        distractor = self.image_embeddings[np.random.choice(distractor_idxs)]\n",
        "\n",
        "        hint = self.keyword_extract(hint)\n",
        "        hint_inputs = clip_processor(\n",
        "            text=hint,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hint_embedding = clip_model.get_text_features(**hint_inputs)\n",
        "            hint_embedding = F.normalize(hint_embedding, p=2, dim=-1).cpu()\n",
        "\n",
        "        target_embedding = self.image_embeddings[target]\n",
        "        return hint_embedding.squeeze(0), target_embedding.squeeze(0), distractor.squeeze(0)\n",
        "\n",
        "class DixitModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout_rate, negative_slope=0.01):\n",
        "        super(DixitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, embedding_dim)\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def forward(self, hint_embedding):\n",
        "        x = F.leaky_relu(self.bn1(self.fc1(hint_embedding)), negative_slope=self.negative_slope)\n",
        "        x = self.dropout1(x)\n",
        "        x = x + hint_embedding\n",
        "        x = F.leaky_relu(self.bn2(self.fc2(x)), negative_slope=self.negative_slope)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    torch.save(state, os.path.join(checkpoint_dir, filename))\n",
        "    if is_best:\n",
        "        torch.save(state, os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "        best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch})\")\n",
        "        return start_epoch, best_val_loss\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from scratch.\")\n",
        "        return 0, float('inf')\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return F.cosine_similarity(a, b, dim=-1)\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_similarities = []\n",
        "\n",
        "    for hint_embedding, target_embedding, distractor_embedding in tqdm(dataloader):\n",
        "        hint_embedding = hint_embedding.to(device)\n",
        "        target_embedding = target_embedding.to(device)\n",
        "        distractor_embedding = distractor_embedding.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor = model(hint_embedding)\n",
        "        loss = criterion(anchor, target_embedding, distractor_embedding)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        similarity_to_target = cosine_similarity(anchor, target_embedding)\n",
        "        similarity_to_distractor = cosine_similarity(anchor, distractor_embedding)\n",
        "\n",
        "        correct_predictions = (similarity_to_target > similarity_to_distractor).sum().item()\n",
        "        total_correct += correct_predictions\n",
        "        total_samples += hint_embedding.size(0)\n",
        "\n",
        "        all_similarities.append(similarity_to_target.mean().item() - similarity_to_distractor.mean().item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_similarity_diff = np.mean(all_similarities)\n",
        "\n",
        "    return avg_loss, accuracy, avg_similarity_diff\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for hint_embedding, target_embedding, distractor_embedding in dataloader:\n",
        "            hint_embedding = hint_embedding.to(device)\n",
        "            target_embedding = target_embedding.to(device)\n",
        "            distractor_embedding = distractor_embedding.to(device)\n",
        "\n",
        "            anchor = model(hint_embedding)\n",
        "            loss = criterion(anchor, target_embedding, distractor_embedding)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            similarity_to_target = cosine_similarity(anchor, target_embedding)\n",
        "            similarity_to_distractor = cosine_similarity(anchor, distractor_embedding)\n",
        "\n",
        "            correct_predictions = (similarity_to_target > similarity_to_distractor).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += hint_embedding.size(0)\n",
        "\n",
        "            all_similarities.append(similarity_to_target.mean().item() - similarity_to_distractor.mean().item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_similarity_diff = np.mean(all_similarities)\n",
        "\n",
        "    return avg_loss, accuracy, avg_similarity_diff\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser_train.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "csv_path = config[\"csv_path\"]\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "checkpoint_dir = config[\"checkpoint_dir\"]\n",
        "embeddings_path = config[\"embeddings_path\"]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001 # Tried 0.001\n",
        "EPOCHS = 50\n",
        "DROPOUT_RATE = 0.6 # Increased from 0.5\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "WEIGHT_DECAY = 1e-5 # Tried 1e-5\n",
        "TRIPLET_MARGIN = 0.4  # Increased from 0.3\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(config[\"model_name\"]).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(config[\"model_name\"])\n",
        "\n",
        "with open(embeddings_path, 'rb') as f:\n",
        "    image_embeddings = pickle.load(f)\n",
        "\n",
        "dataset = DixitDataset(csv_path, image_embeddings)\n",
        "train_size = int((1 - VALIDATION_SPLIT) * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "embedding_dim = next(iter(train_loader))[0].shape[1]\n",
        "model = DixitModel(embedding_dim, DROPOUT_RATE).to(device)\n",
        "criterion = nn.TripletMarginLoss(margin=TRIPLET_MARGIN)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "start_epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_dir)\n",
        "early_stopping_counter = 0\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    train_loss, train_accuracy, train_similarity_diff = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy, val_similarity_diff = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Training Similarity Difference: {train_similarity_diff:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
        "          f\"Validation Similarity Difference: {val_similarity_diff:.4f}\")\n",
        "\n",
        "    is_best = val_loss < best_val_loss\n",
        "    if is_best:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        print(f\"Early stopping counter: {early_stopping_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss\n",
        "    }, is_best, checkpoint_dir)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with open(embeddings_path, 'rb') as f:\n",
        "    image_embeddings = pickle.load(f)\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "class DixitDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_embeddings, device, debug_size=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        if debug_size:\n",
        "            self.data = self.data.sample(n=debug_size, random_state=42)\n",
        "        self.image_embeddings = image_embeddings\n",
        "        self.device = device\n",
        "        self.truncated_count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def keyword_extract(self, text):\n",
        "        words = re.findall(r'\\w+', text.lower())\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        most_common_words = [word for word, _ in Counter(words).most_common(77)]\n",
        "        return ' '.join(most_common_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hint = self.data.iloc[idx]['DESCRIPTION']\n",
        "        target = int(self.data.iloc[idx]['TARGET']) - 1\n",
        "        distractor_idxs = [idx for idx in range(len(self.image_embeddings)) if idx != target]\n",
        "        distractor = torch.tensor(self.image_embeddings[np.random.choice(distractor_idxs)], device=self.device)\n",
        "\n",
        "        hint = self.keyword_extract(hint)\n",
        "        hint_inputs = clip_processor(\n",
        "            text=hint,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hint_embedding = clip_model.get_text_features(**hint_inputs)\n",
        "            hint_embedding = F.normalize(hint_embedding, p=2, dim=-1)\n",
        "\n",
        "        target_embedding = torch.tensor(self.image_embeddings[target], device=self.device)\n",
        "\n",
        "        return hint_embedding.squeeze(0), target_embedding, distractor\n",
        "\n",
        "class DixitModel(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(DixitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.fc2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, hint_embedding):\n",
        "        output = self.fc1(hint_embedding)\n",
        "        output = self.relu(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.normalize(output, p=2, dim=-1)\n",
        "        return output\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        if positive.dim() == 3:\n",
        "            positive = positive.squeeze(1)\n",
        "        if negative.dim() == 3:\n",
        "            negative = negative.squeeze(1)\n",
        "\n",
        "        assert anchor.dim() == 2 and positive.dim() == 2 and negative.dim() == 2\n",
        "\n",
        "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
        "        neg_sim = F.cosine_similarity(anchor, negative, dim=1) / self.temperature\n",
        "\n",
        "        logits = torch.stack([pos_sim, neg_sim], dim=1)\n",
        "\n",
        "        labels = torch.zeros(anchor.size(0), dtype=torch.long, device=anchor.device)\n",
        "\n",
        "        return F.cross_entropy(logits, labels)\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    torch.save(state, os.path.join(checkpoint_dir, filename))\n",
        "    if is_best:\n",
        "        torch.save(state, os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_dir, filename=\"checkpoint.pth\"):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "        best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "        print(f\"Loaded checkpoint '{checkpoint_path}' (epoch {start_epoch})\")\n",
        "        return start_epoch, best_val_loss\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from scratch.\")\n",
        "        return 0, float('inf')\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return F.cosine_similarity(a, b, dim=-1)\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    similarity_diffs = []\n",
        "\n",
        "    for hint_embedding, target_embedding, distractor_embedding in tqdm(dataloader):\n",
        "        hint_embedding = hint_embedding.to(device)\n",
        "        target_embedding = target_embedding.squeeze(1).to(device)\n",
        "        distractor_embedding = distractor_embedding.squeeze(1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        transformed_hint = model(hint_embedding)\n",
        "\n",
        "        loss = criterion(transformed_hint, target_embedding, distractor_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pos_sim = F.cosine_similarity(transformed_hint, target_embedding)\n",
        "        neg_sim = F.cosine_similarity(transformed_hint, distractor_embedding)\n",
        "        correct_predictions = (pos_sim > neg_sim).sum().item()\n",
        "        total_correct += correct_predictions\n",
        "        total_samples += hint_embedding.size(0)\n",
        "\n",
        "        similarity_diffs.append((pos_sim - neg_sim).mean().item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_similarity_diff = np.mean(similarity_diffs)\n",
        "    return avg_loss, accuracy, avg_similarity_diff\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    similarity_diffs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for hint_embedding, target_embedding, distractor_embedding in dataloader:\n",
        "            hint_embedding = hint_embedding.to(device)\n",
        "            target_embedding = target_embedding.squeeze(1).to(device)\n",
        "            distractor_embedding = distractor_embedding.squeeze(1).to(device)\n",
        "\n",
        "            transformed_hint = model(hint_embedding)\n",
        "            loss = criterion(transformed_hint, target_embedding, distractor_embedding)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pos_sim = F.cosine_similarity(transformed_hint, target_embedding)\n",
        "            neg_sim = F.cosine_similarity(transformed_hint, distractor_embedding)\n",
        "            correct_predictions = (pos_sim > neg_sim).sum().item()\n",
        "            total_correct += correct_predictions\n",
        "            total_samples += hint_embedding.size(0)\n",
        "\n",
        "            similarity_diffs.append((pos_sim - neg_sim).mean().item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_similarity_diff = np.mean(similarity_diffs)\n",
        "    return avg_loss, accuracy, avg_similarity_diff\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "config_path = \"/content/drive/My Drive/Colab Notebooks/dixit/config_guesser_train.json\"\n",
        "config = load_config(config_path)\n",
        "\n",
        "csv_path = config[\"csv_path\"]\n",
        "cards_folder = config[\"cards_folder\"]\n",
        "checkpoint_dir = config[\"checkpoint_contrastiveloss_dir\"]\n",
        "embeddings_path = config[\"embeddings_path\"]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001 # Tried 0.001\n",
        "EPOCHS = 50\n",
        "DROPOUT_RATE = 0.5 # Increased from 0.5\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "WEIGHT_DECAY = 1e-5 # Tried 1e-5\n",
        "TRIPLET_MARGIN = 0.4  # Increased from 0.3\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(config[\"model_name\"]).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(config[\"model_name\"])\n",
        "\n",
        "dataset = DixitDataset(csv_path, image_embeddings, device)\n",
        "train_size = int((1 - VALIDATION_SPLIT) * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "embedding_dim = next(iter(train_loader))[0].shape[1]\n",
        "model = DixitModel(embedding_dim).to(device)\n",
        "criterion = NTXentLoss(temperature=0.5)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "start_epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_dir)\n",
        "\n",
        "early_stopping_counter = 0\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    train_loss, train_accuracy, train_similarity_diff = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_accuracy, val_similarity_diff = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Training Similarity Difference: {train_similarity_diff:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
        "          f\"Validation Similarity Difference: {val_similarity_diff:.4f}\")\n",
        "\n",
        "    is_best = val_loss < best_val_loss\n",
        "    if is_best:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        print(f\"Early stopping counter: {early_stopping_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss\n",
        "    }, is_best, checkpoint_dir)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sHgSHKke5aTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMKDKH2NoF2Pa1H3WQLFYPd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}