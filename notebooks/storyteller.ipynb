{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPnBBdS8LzGJWAdNd3bHJ7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumdauo/dixit-AI-bot/blob/main/storyteller.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLxXahbqBt1O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pillow pandas"
      ],
      "metadata": {
        "id": "nu4j3lTdB5-S",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Llama model"
      ],
      "metadata": {
        "id": "T5upqqginAmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import drive\n",
        "from google.colab import runtime\n",
        "from huggingface_hub import login\n",
        "\n",
        "def load_config(config_path='config.json'):\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = json.load(config_file)\n",
        "    return config\n",
        "\n",
        "def load_llama_model(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "    return tokenizer, model\n",
        "\n",
        "config_path = '/content/drive/MyDrive/Colab Notebooks/dixit/config_storyteller.json'\n",
        "config = load_config(config_path)\n",
        "login(config[\"huggingface_token\"])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer, model = load_llama_model(config[\"model_name\"])"
      ],
      "metadata": {
        "id": "q1lRD4xeKpqj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performes the storyteller phase\n",
        "Generates a hint based on the card descriptions"
      ],
      "metadata": {
        "id": "8AuWQWGsnEqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def load_image_by_id(image_id, image_folder):\n",
        "    image_path_1 = os.path.join(image_folder, f\"card_{image_id}.png\")\n",
        "    image_path_2 = os.path.join(image_folder, f\"{image_id}.png\")\n",
        "    if os.path.exists(image_path_1):\n",
        "        image = Image.open(image_path_1)\n",
        "        return image\n",
        "    elif os.path.exists(image_path_2):\n",
        "        image = Image.open(image_path_2)\n",
        "        return image\n",
        "    else:\n",
        "        print(\"Image not found.\")\n",
        "        return None\n",
        "\n",
        "def get_descriptions_from_csv(image_id, csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    row_1 = df[df['Image'] == f\"card_{image_id}.png\"]\n",
        "    row_2 = df[df['Image'] == f\"{image_id}.png\"]\n",
        "    if not row_1.empty:\n",
        "        row = row_1\n",
        "    elif not row_2.empty:\n",
        "        row = row_2\n",
        "    else:\n",
        "        print(\"Descriptions not found for this image ID.\")\n",
        "        return None\n",
        "\n",
        "    descriptions = {\n",
        "        \"BLIP\": row['BLIP'].values[0],\n",
        "        \"ViT\": row['ViT'].values[0],\n",
        "        \"BLIP-2\": row['BLIP-2'].values[0]\n",
        "    }\n",
        "    return descriptions\n",
        "\n",
        "def generate_hint_1(descriptions, tokenizer, model):\n",
        "    max_new_tokens = 15\n",
        "    temperature = 0.9\n",
        "    top_p = 0.8\n",
        "    top_k = 50\n",
        "    repetition_penalty = 1.5\n",
        "    num_beams = 2\n",
        "\n",
        "    examples = (\n",
        "        \"Examples of poetic, standalone clues:\\n\"\n",
        "        \"1. Context: A woman stands alone by the sea\\n   Clue: \\\"Solitude.\\\"\\n\"\n",
        "        \"2. Context: An owl watches in the night\\n   Clue: \\\"Should I fly?\\\"\\n\"\n",
        "        \"3. Context: Flowers bloom under a stormy sky\\n   Clue: \\\"Petals defyh.\\\"\\n\\n\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"{examples}\"\n",
        "        \"Create a short, mysterious clue based on the essence of this scene:\\n\"\n",
        "        f\"{descriptions['BLIP']}\\n\"\n",
        "        f\"{descriptions['ViT']}\\n\"\n",
        "        f\"{descriptions['BLIP-2']}\\n\\n\"\n",
        "        \"Generate a single phrase ending with a period.\\n\"\n",
        "        \"Clue: \"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        num_beams=num_beams,\n",
        "        do_sample=True\n",
        "    )\n",
        "    hint = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Clue: \")[-1].strip()\n",
        "    hint = re.sub(r'[^\\w\\s,.?!]', '', hint)\n",
        "\n",
        "    return hint\n",
        "\n",
        "def generate_hint_2(descriptions, tokenizer, model):\n",
        "    max_new_tokens = 15\n",
        "    temperature = 0.9\n",
        "    top_p = 0.9\n",
        "    top_k = 90\n",
        "    repetition_penalty = 1.3\n",
        "    num_beams = 1\n",
        "\n",
        "    examples = (\n",
        "        \"Examples of poetic clues:\\n\"\n",
        "        \"1. Context: A moonlit forest path\\n   Clue: \\\"Footsteps\\\"\\n\"\n",
        "        \"2. Context: A candle in the dark\\n   Clue: \\\"Shadowâ€™s embrace.\\\"\\n\"\n",
        "        \"3. Context: A river winding through quiet mountains\\n   Clue: \\\"There is just one.\\\"\\n\\n\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"{examples}\"\n",
        "        \"Now, create a short, mysterious clue based on this context:\\n\"\n",
        "        f\"1. {descriptions['BLIP-2']}\\n\"\n",
        "        f\"2. {descriptions['ViT']}\\n\"\n",
        "        f\"3. {descriptions['BLIP']}\\n\\n\"\n",
        "        \"Generate a clue with a dot at the end.\\n\"\n",
        "        \"Clue: \"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        num_beams=num_beams,\n",
        "        do_sample=True\n",
        "    )\n",
        "    hint = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Clue:\")[-1].strip()\n",
        "    hint = re.sub(r'[^\\w\\s,.?!]', '', hint)\n",
        "\n",
        "    return hint\n",
        "\n",
        "def refine_hint(hint_1, hint_2, tokenizer, model):\n",
        "    max_new_tokens = 8\n",
        "    temperature = 0.9\n",
        "    top_p = 0.9\n",
        "    top_k = 70\n",
        "    repetition_penalty = 1.5\n",
        "    num_beams = 3\n",
        "\n",
        "    prompt = (\n",
        "        \"Given the following concepts, generate a very short and enigmatic hint:\\n\"\n",
        "        f\"Concept 1: {hint_1}\\n\"\n",
        "        f\"Concept 2: {hint_2}\\n\\n\"\n",
        "        \"Generate a very short and enigamtic hint.\\n\"\n",
        "        \"Hint: \"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        num_beams=num_beams,\n",
        "        do_sample=True\n",
        "    )\n",
        "    refined_hint = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Hint: \")[-1].strip()\n",
        "    refined_hint = re.sub(r'[^\\w\\s,.?!]', '', refined_hint)\n",
        "\n",
        "    return refined_hint\n",
        "\n",
        "def select_final_hint(hint_1, hint_2, refined_hint):\n",
        "    hint_1 = clean_hint(hint_1)\n",
        "    hint_2 = clean_hint(hint_2)\n",
        "    refined_hint = clean_hint(refined_hint)\n",
        "\n",
        "    if len(refined_hint) > 2:\n",
        "        final_hint = refined_hint\n",
        "    elif len(hint_1) > 2:\n",
        "        final_hint = hint_1\n",
        "    elif len(hint_2) > 2:\n",
        "        final_hint = hint_2\n",
        "    else:\n",
        "        final_hint = \"No hint available.\"\n",
        "\n",
        "    return final_hint\n",
        "\n",
        "def clean_hint(hint):\n",
        "    hint = re.sub(r'^[.\\s]+', '', hint)\n",
        "    hint = re.sub(r'[.\\s]+$', '', hint)\n",
        "    return hint\n",
        "\n",
        "image_id = input(\"Enter the image ID (e.g., 3 for card_3.png): \")\n",
        "image = load_image_by_id(image_id, config[\"image_folder\"])\n",
        "if image:\n",
        "    display(image)\n",
        "    descriptions = get_descriptions_from_csv(image_id, config[\"csv_path\"])\n",
        "    if descriptions:\n",
        "        print(\"Descriptions:\")\n",
        "        for key, desc in descriptions.items():\n",
        "            print(f\"{key}: {desc}\")\n",
        "        hint_1 = generate_hint_1(descriptions, tokenizer, model)\n",
        "        print(\"\\nGenerated Hint 1:\")\n",
        "        print(hint_1)\n",
        "        hint_2 = generate_hint_2(descriptions, tokenizer, model)\n",
        "        print(\"\\nGenerated Hint 2:\")\n",
        "        print(hint_2)\n",
        "        refined_hint = refine_hint(hint_1, hint_2, tokenizer, model)\n",
        "        print(\"\\nRefined Hint:\")\n",
        "        print(refined_hint)\n",
        "        final_hint = select_final_hint(hint_1, hint_2, refined_hint)\n",
        "        print(\"\\nFinal Hint:\")\n",
        "        print(final_hint)\n"
      ],
      "metadata": {
        "id": "04ODJSrwKziy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
